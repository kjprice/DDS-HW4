---
title: "Homework 4 - bootstrap"
output: pdf_document
---

# Bootstrap Normal Distribution

First we create the two normal distributions called `normal.distribution1` and `normal.distribution2`.

```{r}
normal.distribution1 = rnorm(100)
normal.distribution2 = rnorm(1000)
```

## Histograms of Normal Distributions

```{r hist.Normal.Distribution1, echo=FALSE}
hist(normal.distribution1)
```

We would expect the first distribution to have a relatively normal distribution with 100 observations.

```{r hist.Normal.Distribution2, echo=FALSE}
hist(normal.distribution2)
```

And the second distribution to be more normal than the first with 100 observations.

### Bootstrap function

Now we will introduce a bootstrap function that will take the `distribution` as its only argument and will return a new distribution based on creating 1000 means from 50 random samples out of the original distribution.

```{r bootstrap}
bootstrap = function(distribution) {
  # sample size
  n = 50
  #number of simulations
  nsim = 1000
  lotsa.means = numeric(nsim)
  for (i in 1:nsim) {
    x = sample(distribution, 50)
    lotsa.means[i] = mean(x)
  }
  
  lotsa.means
}

```

Let's see what happens when we apply the `bootstrap` method against our `normal.distribution1` and `normal.distribution2` variables and graph the results:


```{r Distribution1, fig.width=4, echo=FALSE}
bootstrap.distribution1 = bootstrap(normal.distribution1)

hist(bootstrap.distribution1, main = "")
```
```{r Distribution2, fig.width=4, echo=FALSE}
bootstrap.distribution2 = bootstrap(normal.distribution2)

hist(bootstrap.distribution2, main = "")
```

As we can see, the bootstrap distribution is even more normal than the original and the amount of variance is much smaller in the bootstrap distribution

```{r}
sd(normal.distribution1)
sd(bootstrap.distribution1)
```

# Bootrstrap Exponential Distribution

Now let's create two exponential distributions and apply the same steps that we did with the normal distributions above.

```{r}
exp.distribution1 = rexp(100)
exp.distribution2 = rexp(1000)
```

As we can see, the data is heavily skewed with a strong, right-tailed distribution for both sets.

```{r, echo=FALSE, fig.width=4}
hist(exp.distribution1, main="")
```
```{r, echo=FALSE, fig.width=4}
hist(exp.distribution2, main="")
```

Applying the `bootstrap` method against our `exp.distribution` variables, we can see that the new distributions are almost completely normal...an amazing transformation from the original, right-tailed distributions.


```{r}
exp.bootstrap.distribution1 = bootstrap(exp.distribution1)
exp.bootstrap.distribution2 = bootstrap(exp.distribution2)
```

```{r, echo=FALSE, fig.width=4}
hist(exp.bootstrap.distribution1, main="")
```
```{r, echo=FALSE, fig.width=4}
hist(exp.bootstrap.distribution2, main="")
```

Notice that the bootstrap distribution is centered around 1

```{r}
mean(exp.bootstrap.distribution1)
```

Finally, notice that the variance is much less within the bootstrap distribution

```{r}
sd(exp.distribution1)
sd(exp.bootstrap.distribution1)

```

Thus, the true power of the Central Limit Theorem has been revealed and demonstrated. 

> The central limit theorem states that the sampling distribution of the mean of any independent, random variable will be normal or nearly normal, if the sample size is large enough. [stattrek.com](http://stattrek.com/statistics/dictionary.aspx?definition=central%20limit%20theorem)
